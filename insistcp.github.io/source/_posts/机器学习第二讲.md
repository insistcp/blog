---
title: 机器学习第二讲
date: 2017-03-12 19:11:57
tags: [机器学习]
categories: 数学 #文章文类
---

## 一：什么是决策树?
通俗的讲就是讲人做决策的过程树状化了
标准概念：决策树是一颗树构造（可以是二叉树,也可以是非二叉树）每次非叶结点表示一个特征属性上的测试，每一个分支节点表示特征属性在某个值域上的输出，每一个叶节点存放一个类别。每一个叶节点存放一个类别(分支结果)。
使用决策树的过程其实就是从节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到达到叶节点，将叶节点存放的结果作为分类的结果。
<!-- more -->，
## 二：怎么构造这个数（如何选择属性）
目的：节点的纯度越高越好
### 2.1: 信息增益
+ A：熵-表示随机变量的不确定性
	假设样本集合D中K样本所占的比例为Pk的话(k=1....|y|)；课D的信息熵为：
$$Ent(D)=-\sum_{k=1}^y P_k·log(P_k)$$
Ent(D)越小表示集合D的稳定性就越高。D的纯度就好。
+ B：条件熵-表示一个变量在一定条件下的不确定性。
信息增益：熵-条件熵。在一定条件下，信息不确定性减少的程度。
设离散属性a有V个可能的取值[a^1,a^1,....a^V],其中第v个分支节点包含了D中所有在属性上取值为a^v的样本记为D^v。由上面的公式可以计算出D^v的信息熵。考虑的不同的分支节点所包含的样本数量不容再给予一定的权重，那么我们就可以工会通过下式计算出属性a对样本集D进行划分所得到的的“信息增益”
$$Gain(D,a)=Ent(D)-\sum_{v=1}^V{D^v\over D}·Ent(D^v)$$
**信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。所以我们在选择属性时就可以依靠信息增益来选择到底选择哪一个属性来进行划分。著名的ID3算法就是依据信息增益进行属性选择构造决策树的**
eg：我们用随机变量X表示明天要下雨，通过系列已知条件我们可以计算出X的熵。变量Y表示明天是阴天，则变量Y的熵也可以计算出来；如果我们可以计算出阴天的状况下下雨的熵---就是条件熵。那么两者相减得出的结果就是信息增益。就是在明天是阴天的条件下，明天下雨的不确定程度。加入我们通过计算得出明天下雨的熵是2，明天是阴天的情况下下雨的熵是0.01，则信息增益就是2-0.01=1.99；得出信息增益很大，进而得出阴天这个属性对下雨来说是很重要的。
### 2.2 信息增益率：
大家想一想如果我们使用信息增益来进行属性划分时，如果选择一个取值连续的属性进行划分（每一个样本独有一个）那么将得到很好的信息增益（可以理解为编号将产生17个分支每一个分支只有一个样本，所以每一个分支的村纯度已经达到了最大即上面的式子D^v取得最大-上述公式的最后一项为0）但是采用编号这个属性划分明显不行，因为它并没有很好的适应新样本的能力，即来一个新样本我们就增加一个新分支。泛型差。
所以我们提出了信息增益率，就是为了弥补使用信息增益选择属性时偏爱属性取值多的属性的不足。
$$Info = -\sum_{v=1}^V{D^v\over D}·log_2{D^v\over D}$$
我们将Info称为一个属性的固有值一般情况下，我们属性的取值越多我们info取值就越大注意令x=D^v/D则IV是一个关于x的递减函数，x取值越小则函数值越大。
$$Gain\\_ratio(D,a) = {Gain(D,a)\over IV(a)}$$
由此我们可以看出信息增益率更加偏爱属性取值较少的属性，所以我们的C45算法在ID3算法上进行改进，先从候选属性中选择信息增益量高于平均水平的属性，在从中选择增益率最高的属性进行划分。

### 2.3：基尼指数：
CART(Classfiation and Regression Tree)使用的是基尼指数选择属性划分的，数据集D的纯度可用以下式子衡量
$$Gini(D)=1-\sum_{k=1}^y{P_k^2}$$

其实表示的物理意义就是从集合D中随机取两个样本他们的最终类别标志不一致的概率
$$Gini_index(D,a)=\sum_{v=1}^V {D^v\over D}{Gini(D^v)}$$
所以我们就选择属性集合A中，基尼指数最小的属性九尾最优划分属性。
### 2.4：剪枝处理
**剪枝是决策树中为了解决过拟合的手段**
+ 1：预剪枝
	是否要采用该属性进行划分我们需要对比划分前后的泛型（即精准度），划分之前我们算出部采用该属性进行划分时的测试集的精准度，采用该属性进行划分的测试机的精准度做一对比，再决定是否要进行划分
+ 2：后剪枝
	先从训练集中训练出来一个决策树，然后自底向上，对每一个非叶节点(将其领衔的叶节点删除，并将其变为叶节点)在促使集上进行判断，决定是否要进行剪枝。
## 三：连续和缺失值
### 3.1：连续值处理
	对于一个属性是连续值时将其从大到小进行排序后的得到{a1,a2...an}我们需要选择一个节点对这一堆连续值进行划分，
	$$T^a ={a^i+a^{i+1}\over{2}}   1<=i<=n-1$$
### 3.2：缺失值处理

## 四：多变量决策树 
+ 多变量决策树其实就是每一个属性不再是有个属性值决定而是由多个属性的线性组合决定的
