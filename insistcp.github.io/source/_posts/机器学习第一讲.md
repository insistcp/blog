---
title: 机器学习第一讲
date: 2017-03-12 14:53:34
tags: [机器学习]
categories: 技术 #文章文类
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
## 一：什么是机器学习
### 1.1：概念
__引言：让我们一起来回忆我们曾经买西瓜的经历，先看个头，然后敲一敲，再看看根蒂，然后做出我们的判断，好瓜还是坏瓜。仔细想想我们判断的过程，西瓜的大小，声音，根蒂，通过着一系列的西瓜的属性告诉我们它是好瓜还是坏瓜，为什么这些西瓜的属性可以判断它是好瓜还是坏瓜呢，不错就是**经验**，以往的经验告诉我们的，其实某种意义上讲这就是机器学习，根据以前的经验学习（训练样本）为以后的选择（预测）做判断。
<!-- more -->，
下面看几个标准定义：
+ 机器学习：研究如何通过计算的手段，利用经验来改善系统自身的性能。
+ 机器学习的本质是空间搜索和函数的泛化
一个好的机器学习问题：一个程序被认为可以从经验E中学习，解决任务T，达到性能P，当且仅当，有了经验E后，经过了P的评判，程序在处理T时的性能有所提升。
Eg：拿围棋举例，
E就是程序上万次的自我练习的经验，T任务就是下棋，P就是和一些新手比赛时，赢得比赛的概率。
### 1.2：机器学习的分类（按照学习方式）
- A：监督学习
	+ 训练的数据是有标签的。
	+ 每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，
	+ 监督式学习的常见应用场景：分类问题和回归问题
- B：非监督学习
	+ 训练的数据是没有标签的；
	+ 用于发掘样本内在结构的联系
	+ 主要用于聚类，关联规则学习：例如 ，我们有一堆球，颜色不同，尺寸也不同；
- C：半监督学习
	+ 训练的数据部分是有标签的部分是没有标签
	+ 模型主要用来预测，先要学习样本内部结构联系
	+ 应用场景：回归和分类
- D：强化学习
	+ 输入的数据作为模型的反馈，并且模型在接受输入必须做出反馈。
	+ 应用领域：动态系统，机器人控制等

### 1.3：机器学习知识图谱
[引自知乎](https://zhuanlan.zhihu.com/p/25075477)
<center>![机器学习知识图谱：引子知乎](/机器学习第一讲/1.png)</center>

## 二：模型评估
### 2.1：经验误差和过拟合
+ 经验误差：学习器在训练集上的误差
+ 泛化误差：学习器在新样本上的误差
+ 过拟合：（学习能力过强）学习器将训练样本本身的特点当做所有潜在样本都具有的性质过拟合，将会导致其泛化能力差（适应新样本的能力差） 
+ 欠拟合：(学习能力过弱)
<center>![过拟合和欠拟合直观表先](/机器学习第一讲/2.jpg)</center>
### 2.2:评估方法---计算模型的泛化误差。
+ 1：留出法：直接将数据集D划分为两个互斥的结合其中一个作为训练集S另一个作为测试集T。在S上练出模型之后，用T来评估其测试误差，作为泛化误差的估计	
+ 2：交叉验证法：将数据集划分D为k个大小相同的互斥子集。D=D1UD2U…..Dk  每次用k-1个子集作为训练集，余下的那个作为测试集。这样就可以获得k组训练测试集。
<center>![交叉验证法](/机器学习第一讲/3.jpg)</center>
+ 3：自助法
每次从样本集合D中挑选一个Dj,将其Copy一份再放回，然后再取，这样进行m次就得到m个样本的数据集D’。将剩余的作为测试样本。
+ 4：调参和最终模型
### 2.3：性能度量
+ 1：错误率和精度
+ 2：查准率和查全率
+ 3：ROC（受试者工作特征）和AUC（Area Under ROC Curve ROC曲线围成的面积）
+ 4：代价敏感错误率和代价曲线
### 2.4：比较检验
+ 1：假设检验
+ 2：交叉验证t检验
+ 3：McNemar检验
+ 4：Friedman检验和Nemenyi后续检验
### 2.5：偏差和方差
+ Variance(方差)：估计本身的方差。
+ Bias（偏差）：估计的期望和样本数据样本希望得到的回归函数之间的差别。
举个例子：具体来讲，我有一个统计量D（比如统计某大学研一学生身高在[0.5-1],[1,1.1],[1.1,1.2]……[1.9,2]之间的人数），这样可以形成一些离散点。然后呢，本校研一有20个班，每个班就都可以拟合成一条估计曲线f(x)，这20条曲线呢，有一个平均值，也就是估计期望（均值）曲线E(f(x,D))。

**variance是指，这20条估计曲线与最后估计期望（均值）之间的距离，也就是估计曲线本身的方差，是不可能为0的。**
**bias是指，20条估计曲线的均值与实际最佳拟合情况之间的距离。**
<center>![方差和偏差](/机器学习第一讲/4.jpg)</center>

### 三：线性回归
#### 3.1：基本概念：
根据已经有的样本数据拟合出出来一条直线对连续的数据集进行预测
+ A：损失函数：损失函数是一种衡量损失和错误（这种损失与“错误地”估计有关，如费用或者设备的损失）程度的函数，说白了就是一种衡量我们所拟合的函数好坏的一种函数。
$$ h𝜃(x) = 𝐴=𝜃0+𝜃1∗𝑋 $$     
<center>![公式1](/机器学习第一讲/A.png)</center>
例子：预测房价，假设你现在有重庆房价和其房子大小关系的一系列数据，如下图
我们的目标使J(𝜃0,𝜃1)最小！！！
<center>![公式1](/机器学习第一讲/5.jpg)</center>

#### 3.2：梯度下降法
<center>![公式1](/机器学习第一讲/b.png)</center>
开始我们假设：𝜃0=0，则
<center>![公式1](/机器学习第一讲/c.png)</center>
Target  ： Minimize(J(𝜃1)) 得下图：
<center>![梯度下降图](/机器学习第一讲/6.jpg)</center>
我们知道当𝜃1 = 1时，取得最小值。
当𝜃0！=0时呢？
<center>![公式1](/机器学习第一讲/b.png)</center>
当𝜃0！=0时呢？ J（𝜃）图像就是一个三维的，如右图：
<center>![公式1](/机器学习第一讲/7.jpg)</center>
可以看出最低点就是我们要求的点；
**问题？我们怎么样去找到这个点呢？**
于是便提出了*梯度下降（导数下降）*
gradient descent是指梯度下降，为的是将costfunciton 描绘出之后，让参数沿着梯度下降的方向走，并迭代地不断减小J(theta0，theta1)直到达到一个稳态（J不再减小）
梯度下降：
如下图，我们绘制出来的cost函数，
<center>![公式1](/机器学习第一讲/8.jpg)</center>
加入我们的起点选择在图中的黑圆圈的位置，沿着梯度下降的方式（如图的黑线）走，我们就能找到一个最小值（局部）所以我们可以得到一个参数的更新公式：
其中的 ɑ称之为学习率；
一样我们先来看看最简单的情况：
假设𝜃0=0
<center>![公式1](/机器学习第一讲/9.jpg)</center>
<center>![公式1](/机器学习第一讲/10.jpg)</center>

同理当𝜃0！=0时，我们再来推，每得到一次参数值我们就计算一个J值，知道J值不再减小，此时的参数值就to是我们要求的值
<center>![公式1](/机器学习第一讲/11.jpg)</center>
<center>![公式1](/机器学习第一讲/12.jpg)</center>

## 四：逻辑回归
### 4.1：一些概念
我们先来看一个例子
例子：根据肿瘤的尺寸预测病人是否患有癌症（分类问题）
如果我们使用线性回归去拟合对于第一幅图的数据集表现还可以
线性回归：h𝜃(x) = 𝐴=𝜃0+𝜃1∗𝑋
<center>![公式1](/机器学习第一讲/13.jpg)</center>
但是当我们在右上角添加一个元素我们会发现线性回归的模型你不能很好的去解决分类问题：，如下图所示
<center>![公式1](/机器学习第一讲/14.jpg)</center>
于是我们便提出了线性回归：
它的目的：分类
一般情况下，它的输出是离散点的，最为直观的要数二分类问题；就像上面的预测病人是否患有癌症，输出结果只有两种情况：患有癌症，没有患癌症；最早解决这类问题提出来的模型是感知器：对所有的输入特征和权重做点积，将计算的结果和我们所设置的阈值作比较，从而将样本分为两类。
<center>![感知机](/机器学习第一讲/17.jpg)</center>
<center>![对应的函数](/机器学习第一讲/18.jpg)</center>

从图中可以看出感知机模型存在两个明显的缺点：
+ 1：如果我们新来一个点为t0+0.01;那么这个点的输出是0还是1呢
+ 2：函数在t0位置时有一个阶跃，从0-1的突变，导致这个点不连续，应用时对这个点的处理极不方便。
于是我们便提出了Sigmod函数，如下图：
<center>![公式1](/机器学习第一讲/19.jpg)</center>
它具有以下两个特点：
+ 1：它的输入从正无穷大到负无穷大对应的输出刚好到0-1之间，正好满足（0，1）分布。使用概率去描述分类器，相对于定阈值要方便很多。
+ 2：单调连续的上升函数，具有很好的连续性。 
模型如下图所示：
<center>![公式1](/机器学习第一讲/15.jpg)</center>

+ Sigmoid function:
所谓Sigmoid function或Logistic function就是这样一个函数g(z)见上图所示
当z>=0时，g(z)>=0.5；（表示y=1）
当z<0时，g(z)<0.5（表示y=0）
由下图中公式知，给定了数据x和参数θ，y=0和y=1的概率和=1
<center>![公式1](/机器学习第一讲/16.jpg)</center>

### 4.2：逻辑回归的损失函数
<center>![公式1](/机器学习第一讲/21.jpg)</center>
上面已经讲了，H(x)计算的出来的结果可以理解为概率P，那么：
H(x1)就是x=x1的条件下y=y1的概率p1，
H(x2)就是x=x2的条件下y=y2的概率p2， 
....
H(xn)就是x=xn的条件下y=yn的概率pn，
对于已经有的样本（X,Y）和h(x)是什么样的函数是结果最理想呢，
就是当P(1)p(2)....P(n)取最大的情况下最好，而损失函数一般为每条数据的损失之和，故而取对数将乘积变成和的形式。
加上符号就是为了让最大似然值和最小损失对应起来

### 4.3：使用梯度下降计算参数值：
+ 和线性回归一样，采用梯度下降的方式进行参数的更新
<center>![公式1](/机器学习第一讲/22.jpg)</center>
 每一个的参数更新方式如下所示
<center>![公式1](/机器学习第一讲/23.jpg)</center> 



